# Docker Offload - Large Model Override
version: '3.8'

services:
  backend:
    # Override to use the large model for offload scenarios
    models: !override
      qwen3-large:
        endpoint_var: MODEL_RUNNER_URL
        model_var: MODEL_RUNNER_MODEL

# Large model for Docker Offload scenarios requiring maximum performance
models:
  qwen3-large:
    model: ai/qwen2.5-coder:14B-Q4_K_M  # 8.5 GB
    context_size: 15000  # 12 GB VRAM
    # increase context size to handle larger results
    # context_size: 32000  # 20 GB VRAM
    runtime_flags:
      - "--threads=8"
      - "--ctx-size=15000"
      - "--batch-size=512"
      - "--mlock"