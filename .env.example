# Smart Code Assistant - Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# API Configuration
# =============================================================================

# Backend API URL (used by frontend)
REACT_APP_API_URL=http://localhost:8000

# Application mode (local or offload)
REACT_APP_MODE=local

# =============================================================================
# Model Configuration
# =============================================================================

# Model size: small (for local) or large (for Docker Offload)
MODEL_SIZE=small

# Model name/path
# For small models (local): Salesforce/codet5-small, microsoft/CodeBERT-base
# For large models (offload): codellama/CodeLlama-7b-Instruct-hf, codellama/CodeLlama-13b-Instruct-hf
MODEL_NAME=Salesforce/codet5-small

# Maximum tokens to generate
MAX_TOKENS=100

# =============================================================================
# Hardware Configuration
# =============================================================================

# Use GPU acceleration (true/false)
USE_GPU=false

# PyTorch data type (float32, float16, bfloat16)
# float16/bfloat16 recommended for GPU, float32 for CPU
TORCH_DTYPE=float32

# CUDA device (0, 1, 2, etc. or 'auto')
CUDA_DEVICE=0

# =============================================================================
# Performance Configuration
# =============================================================================

# Enable model caching
ENABLE_CACHING=true

# Cache directory
CACHE_DIR=./models_cache

# Batch size for inference
BATCH_SIZE=1

# =============================================================================
# Development Configuration
# =============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Enable development mode features
DEVELOPMENT_MODE=true

# API request timeout (seconds)
API_TIMEOUT=30

# =============================================================================
# Docker Configuration
# =============================================================================

# Docker Compose project name
COMPOSE_PROJECT_NAME=smart-code-assistant

# Docker build target (development, production)
DOCKER_TARGET=development

# =============================================================================
# Security Configuration
# =============================================================================

# API rate limiting (requests per minute)
RATE_LIMIT=60

# Enable CORS (for development)
ENABLE_CORS=true

# Allowed origins (comma-separated)
ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# =============================================================================
# Monitoring Configuration
# =============================================================================

# Enable metrics collection
ENABLE_METRICS=true

# Metrics endpoint
METRICS_ENDPOINT=/metrics

# Health check interval (seconds)
HEALTH_CHECK_INTERVAL=30

# =============================================================================
# Example Configurations
# =============================================================================

# Local Development (CPU, Small Model)
# MODEL_SIZE=small
# USE_GPU=false
# MODEL_NAME=Salesforce/codet5-small
# MAX_TOKENS=100
# TORCH_DTYPE=float32

# Docker Offload (GPU, Large Model)
# MODEL_SIZE=large
# USE_GPU=true
# MODEL_NAME=codellama/CodeLlama-7b-Instruct-hf
# MAX_TOKENS=500
# TORCH_DTYPE=float16

# =============================================================================
# Notes
# =============================================================================

# 1. For local development, keep MODEL_SIZE=small and USE_GPU=false
# 2. For Docker Offload, set MODEL_SIZE=large and USE_GPU=true
# 3. Large models require significant GPU memory (8GB+ VRAM)
# 4. Adjust MAX_TOKENS based on your use case and performance requirements
# 5. Use float16 or bfloat16 for better GPU performance with large models