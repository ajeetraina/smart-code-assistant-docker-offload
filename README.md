# Smart Code Assistant with Docker Offload

A progressive AI coding assistant that seamlessly scales from fast local models to powerful Docker Offload models based on your needs.

## ğŸš€ Quick Start

### Prerequisites

1. **Docker Desktop 4.43+** with Model Runner enabled
2. **Mac with Apple Silicon** (M1/M2/M3) for optimal local performance
3. **At least 8GB RAM** available (12GB+ recommended for large models)

### Three-Tier Model System

#### ğŸŸ¢ Small Model (Local GPU)
```bash
# Pull small model for fast development
docker model pull ai/smollm2:1.7B-Q8_0

# Start with small model (default)
docker-compose up --build
```

#### ğŸŸ  Medium Model (GPU Accelerated) 
```bash
# Pull medium model for balanced performance
docker model pull ai/qwen2.5-coder:7B-Q4_0

# Override to use medium model
docker-compose up --build -e MODEL_TIER=medium
```

#### ğŸŸ£ Large Model (Docker Offload)
```bash
# Pull large model for maximum capability
docker model pull ai/qwen2.5-coder:14B-Q4_K_M

# Scale to Docker Offload
docker-compose -f docker-compose.yml -f docker-compose.offload.yml up --build
```

### Access Points

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8080  
- **Health Check**: http://localhost:8080/health

## ğŸ—ï¸ Progressive Architecture

### Tier 1: Local Development
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â–¶â”‚   SmolLM2 1.7B â”‚
â”‚ (React/TS)  â”‚    â”‚ (FastAPI)   â”‚    â”‚   ğŸŸ¢ Local GPU  â”‚
â”‚ Port: 3000  â”‚    â”‚ Port: 8080  â”‚    â”‚   1.5GB â€¢ Fast  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tier 2: GPU Accelerated  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â–¶â”‚   Qwen2.5 7B   â”‚
â”‚ (React/TS)  â”‚    â”‚ (FastAPI)   â”‚    â”‚   ğŸŸ  GPU Accel  â”‚
â”‚ Port: 3000  â”‚    â”‚ Port: 8080  â”‚    â”‚   4.2GB â€¢ Smart â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tier 3: Docker Offload
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â–¶â”‚  Qwen2.5 14B   â”‚
â”‚ (React/TS)  â”‚    â”‚ (FastAPI)   â”‚    â”‚ ğŸŸ£ Docker Offloadâ”‚
â”‚ Port: 3000  â”‚    â”‚ Port: 8080  â”‚    â”‚  8.5GB â€¢ Expert â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš¡ Model Comparison

| Tier | Model | Size | VRAM | Context | Speed | Best For |
|------|-------|------|------|---------|-------|----------|
| ğŸŸ¢ **Small** | SmolLM2 1.7B | 1.5GB | 2GB | 4K tokens | Very Fast | Quick dev, syntax help |
| ğŸŸ  **Medium** | Qwen2.5 7B | 4.2GB | 6GB | 8K tokens | Fast | Balanced coding tasks |
| ğŸŸ£ **Large** | Qwen2.5 14B | 8.5GB | 12GB | 15K tokens | Powerful | Complex generation |

## ğŸ”§ Docker Compose Configuration

### Base Configuration (`docker-compose.yml`)
```yaml
services:
  backend:
    models:
      qwen3-small:
        endpoint_var: MODEL_RUNNER_URL
        model_var: MODEL_RUNNER_MODEL

models:
  qwen3-small:
    model: ai/smollm2:1.7B-Q8_0
    context_size: 4096
  
  qwen3-medium:
    model: ai/qwen2.5-coder:7B-Q4_0
    context_size: 8192
    
  # qwen3-large defined in docker-compose.offload.yml
```

### Offload Override (`docker-compose.offload.yml`)
```yaml
services:
  backend:
    models: !override
      qwen3-large:
        endpoint_var: MODEL_RUNNER_URL
        model_var: MODEL_RUNNER_MODEL

models:
  qwen3-large:
    model: ai/qwen2.5-coder:14B-Q4_K_M
    context_size: 15000
```

## ğŸ¨ Smart UI Features

### Automatic Mode Detection
- **Visual Indicators**: Different icons and colors for each tier
- **Real-time Status**: Shows current model name and performance tier  
- **Connection Monitor**: Live status of Docker Model Runner
- **Performance Info**: Displays model capabilities and context size

### Progressive Enhancement
- **Adaptive Timeouts**: Automatically adjusted based on model size
- **Dynamic Token Limits**: Optimized for each model's capabilities
- **Smart Streaming**: Efficient real-time response generation
- **Error Handling**: Graceful fallbacks with clear error messages

## ğŸ”„ Development Workflow

### 1. Start Fast (Tier 1)
```bash
# Quick development with small model
docker-compose up --build
```

### 2. Scale Smart (Tier 2)  
```bash
# More capable model for complex tasks
# Edit docker-compose.yml to use qwen3-medium
docker-compose up --build
```

### 3. Max Power (Tier 3)
```bash
# Docker Offload for production-level AI
docker-compose -f docker-compose.yml -f docker-compose.offload.yml up --build
```

## ğŸ¯ When to Use Each Tier

### ğŸŸ¢ Use Small Model When:
- âœ… Quick development and prototyping
- âœ… Simple code completion and syntax checking
- âœ… Fast iteration cycles
- âœ… Limited local resources (8GB RAM)
- âœ… Working offline

### ğŸŸ  Use Medium Model When:
- ğŸ”¥ Balanced performance needs
- ğŸ”¥ More complex code generation
- ğŸ”¥ Better context understanding
- ğŸ”¥ Moderate resource availability (12GB RAM)
- ğŸ”¥ Production-quality suggestions

### ğŸŸ£ Use Large Model When:
- ğŸš€ Maximum AI capability required
- ğŸš€ Complex architecture design
- ğŸš€ Large codebase analysis
- ğŸš€ Advanced refactoring tasks
- ğŸš€ Expert-level code generation

## ğŸš¨ Troubleshooting

### Model Not Loading
```bash
# Check available models
docker model list

# Pull specific tier models
docker model pull ai/smollm2:1.7B-Q8_0          # Small
docker model pull ai/qwen2.5-coder:7B-Q4_0      # Medium  
docker model pull ai/qwen2.5-coder:14B-Q4_K_M   # Large

# Verify model info
docker model info ai/smollm2:1.7B-Q8_0
```

### Performance Issues
```bash
# Check system resources
docker system df
docker stats

# Monitor model runner
curl http://localhost:8080/health
curl http://localhost:8080/api/model-info
```

### Switching Between Tiers
```bash
# Stop current setup
docker-compose down

# Switch to different tier
docker-compose -f docker-compose.yml -f docker-compose.offload.yml up --build
```

## ğŸ“Š Environment Variables

Docker automatically injects model configuration:

**All Tiers:**
- `MODEL_RUNNER_URL` - Model endpoint URL
- `MODEL_RUNNER_MODEL` - Current model identifier

**Auto-Detection:**
- Backend automatically detects model size from name
- Frontend shows appropriate tier indicators
- Timeouts and limits adjust automatically

## ğŸ¯ What This Demonstrates

This implementation showcases **progressive AI scaling**:

âœ… **Start Small** - Begin with fast, lightweight models for development  
âœ… **Scale Intelligently** - Move to more capable models as needs grow  
âœ… **Max Performance** - Use Docker Offload for expert-level AI assistance  
âœ… **Same Interface** - No code changes required between tiers  
âœ… **Smart Detection** - Automatic optimization based on current model  
âœ… **Production Ready** - Handles everything from dev to production workloads  

Perfect for teams that want to **start fast** and **scale seamlessly** as their AI assistance needs evolve!

## ğŸ“„ License

MIT License - feel free to modify and distribute!