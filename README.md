# Simple SmolLM2 Chatbot with Mac GPU Support

A clean, simple chatbot interface powered by SmolLM2 running locally on your Mac with GPU acceleration via Docker Model Runner.

## ğŸš€ Quick Start

### Prerequisites

1. **Docker Desktop 4.43+** with Model Runner enabled
2. **Mac with Apple Silicon** (M1/M2/M3) for optimal GPU performance
3. **At least 8GB RAM** available

### Step 1: Pull SmolLM2 Model

```bash
# Pull the optimized SmolLM2 model for Mac
docker model pull ai/smollm2:1.7B-Q8_0
```

### Step 2: Clone and Start

```bash
# Clone this repository
git clone https://github.com/ajeetraina/smart-code-assistant-docker-offload.git
cd smart-code-assistant-docker-offload

# Start the application
docker-compose up --build
```

### Step 3: Access the Chat

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8080
- **Health Check**: http://localhost:8080/health

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â–¶â”‚ Docker Model    â”‚
â”‚ (React/TS)  â”‚    â”‚ (FastAPI)   â”‚    â”‚ Runner (SmolLM2)â”‚
â”‚ Port: 3000  â”‚    â”‚ Port: 8080  â”‚    â”‚ Auto-managed    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš¡ What's Different

### âœ… **Added - Simple & Working**
- **Clean chatbot UI** - Just messages, input, and send button
- **SmolLM2 integration** - Local Mac GPU acceleration  
- **Real-time streaming** - Tokens appear as they're generated
- **Proper Docker Models** - Uses official Compose models syntax
- **Auto-configuration** - Docker injects model URLs automatically

### âŒ **Removed - Complex Demo Stuff**
- Performance metrics dashboards
- Docker Offload comparison charts
- System information cards  
- Example prompt sidebars
- Decision guides about local vs cloud
- Complex tabs and navigation

## ğŸ”§ Configuration

### Docker Compose Models

The `docker-compose.yml` uses the official Docker Compose models specification:

```yaml
services:
  backend:
    models:
      - llm

models:
  llm:
    model: ai/smollm2:1.7B-Q8_0
    context_size: 4096
    runtime_flags:
      - "--threads=8"
      - "--ctx-size=4096"
```

Docker automatically injects these environment variables into the backend:
- `LLM_URL` - URL to access the model
- `LLM_MODEL` - Model identifier

### Model Options

Change the model by updating `docker-compose.yml`:

```yaml
models:
  llm:
    model: ai/smollm2:1.7B-Q8_0  # Fast, recommended
    # model: ai/smollm2:1.7B-Q4_0  # Smaller size
    # model: ai/llama3.2:1B-Q8_0   # Alternative
```

## ğŸ¨ Features

- **Clean Interface** - Simple chat with no clutter
- **Real-time Streaming** - Watch responses generate token by token  
- **Mac GPU Acceleration** - Optimized for Apple Silicon
- **Model Status** - Visual connection status indicator
- **Error Handling** - Graceful fallbacks and clear error messages
- **Health Monitoring** - `/health` endpoint for service status

## ğŸš¨ Troubleshooting

### Model Not Loading

```bash
# Check if model is available
docker model list

# Re-pull if needed
docker model pull ai/smollm2:1.7B-Q8_0

# Check if Model Runner is working
docker model info ai/smollm2:1.7B-Q8_0
```

### Connection Issues

```bash
# Check backend health
curl http://localhost:8080/health

# Check model info endpoint
curl http://localhost:8080/api/model-info

# Check Docker Compose services
docker-compose ps
```

### Performance Issues

- **Slow responses**: Ensure Mac GPU is being utilized
- **Memory errors**: Close other applications to free up RAM  
- **Docker issues**: Restart Docker Desktop

## ğŸ“Š Development

For development with hot reloading:

```bash
# Frontend (in frontend/)
npm install
npm run dev  # Port 3000

# Backend (in backend/)
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8080
```

## ğŸ¯ What This Achieves

This transforms the repository from a **complex Docker Offload demonstration** into a **simple, functional chatbot** that:

âœ… Actually works with local AI models  
âœ… Uses proper Docker Compose models syntax  
âœ… Provides a clean user experience  
âœ… Runs SmolLM2 with Mac GPU acceleration  
âœ… Streams responses in real-time  
âœ… Has proper error handling  

Perfect for developers who want a clean AI coding assistant without technical complexity!

## ğŸ“„ License

MIT License - feel free to modify and distribute!